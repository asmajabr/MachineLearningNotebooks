{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "DIlzJbCpmo0R"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/MachineLearningNotebooks/blob/master/Logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzmEcNHzuBI3",
        "colab_type": "text"
      },
      "source": [
        "# Clone the Source GitHub Reporsitory \n",
        "We need to clone some source files to be used throughtout this tutorial from a GitHub reprository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmP4GrRNudXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./MachineLearning\n",
        "!git clone https://github.com/mkjubran/MachineLearning.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIlzJbCpmo0R",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression - Binary Classification\n",
        "**Introduction**\n",
        "\n",
        "In this section, we will use logistic regression to solve few classification problems. We will focus on binary classification in which the dataset belongs to only two classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeQ0wuIc1AHR",
        "colab_type": "text"
      },
      "source": [
        "**Theory** \\\\\n",
        "\n",
        "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.$^{[1]}$\n",
        "\n",
        "Types of Logistic Regression $^{[1]}$:\n",
        "1. Binary Logistic Regression: The categorical response has only two 2 possible outcomes. E.g.: Spam or Not\n",
        "2. Multinomial Logistic Regression: Three or more categories without ordering. E.g.: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
        "3. Ordinal Logistic Regression: Three or more categories with ordering. E.g.: Movie rating from 1 to 5\n",
        "\n",
        "[1] https://medium.com/greyatom/logistic-regression-89e496433063\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrMDfwQQzBEn",
        "colab_type": "text"
      },
      "source": [
        "**Readings and Resources**\n",
        "\n",
        "[1] https://medium.com/greyatom/logistic-regression-89e496433063"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOQvdG9QW1Yv",
        "colab_type": "text"
      },
      "source": [
        "# Case #1: Studying Hours and Passing Exams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7ietEiOzhNl",
        "colab_type": "text"
      },
      "source": [
        "In this section we will use logistic regression to infer whether a student will pass or fail an exam based on the number of hours the student spends preparing for the exam. A dataset for few students that includes the number of study hours and whether they pass (1) or fail (0) the exam. This is a binary classification problem that can be solved using logistic regression as will be shown next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RSwASngm9_9",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Read the input data (number of study hours and exam pass or fail) from the csv file (HoursPassExam.csv) file. Use the pandas library (https://pandas.pydata.org/) to read the data from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQX2iq_fnJOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./MachineLearning/2_logistic/HoursPassExam.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJA9kY1o6_lr",
        "colab_type": "text"
      },
      "source": [
        "To view the dataset, we will use the scatter plot from matplotlib library as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryh3BJOV7eo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(df['hours'],df['pass'],color = 'red', marker = '+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39M2I3LU70dB",
        "colab_type": "text"
      },
      "source": [
        "As can be seen, the output (y) is binary; 0 for failing the exam and 1 for passing the exam. Also, the chances of passing the exam increases when the number of studying for the exam increases. Let us divide the dataset into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uIoyL855AoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x = df[['hours']]\n",
        "y = df['pass']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print('size of test dataset = {}, size of traing data = {}, percentage = {}%'.format(len(x_test),len(x_train),len(x_test)*100/(len(x_test) + len(x_train))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUTs3Ywp5ARf",
        "colab_type": "text"
      },
      "source": [
        "Let us now try to use linear regresison to fit this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtqBhjVN8Hgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x_train,y_train)\n",
        "print(reg.coef_)\n",
        "print(reg.intercept_)\n",
        "print(reg.score(x_train,y_train))\n",
        "plt.scatter(x_train,y_train,color = 'red', marker = '+', label = 'Data')\n",
        "plt.plot(x_train,reg.predict(x_train) , label = 'Linear')\n",
        "plt.legend()\n",
        "plt.xlabel('Hourse')\n",
        "plt.ylabel('Pass Exam (0:pass,1:fail)')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HHv5Gko8_20",
        "colab_type": "text"
      },
      "source": [
        "As can be observed from the above figure, the linear regresison fails to fit the data (also training accuracy ~= 56%). Let us try the logitic regression to fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBkR7WTW9T7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "logreg = linear_model.LogisticRegression()\n",
        "logreg.fit(x_train,y_train)\n",
        "print(logreg.coef_)\n",
        "print(logreg.intercept_)\n",
        "print(logreg.score(x_train,y_train))\n",
        "print(logreg.score(x_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZplRx6NhjbWq",
        "colab_type": "text"
      },
      "source": [
        "The training accuracy of the logistic regression is about $90.196\\%$ (testing Acc ~= $61.54$) which is higher than the linear regression. This means that the logitic regresison fit the data better. Let us plot the logitic regression curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ9X3woTjbhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(x,y,color = 'red', marker = '+', label = 'Data')\n",
        "plt.plot(x,reg.predict(x) , label = 'Linear')\n",
        "plt.xlabel('Hourse')\n",
        "plt.ylabel('Pass Exam (0:pass,1:fail)')\n",
        "\n",
        "import numpy as np\n",
        "x_sigmoid = np.sort(np.array(x),axis=0)\n",
        "##plot the logistic regression\n",
        "plt.plot(x_sigmoid,logreg.predict(pd.DataFrame(x_sigmoid)), label = 'Logitic', color ='orange')\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q33xV2b7Ab1",
        "colab_type": "text"
      },
      "source": [
        "We observe from the above figure that the logistic regression curve (orange line) fits better the dataset as compared to the linear regression curve (blue line). Let us also plot the sigmoid curve on the same figure using the logistic coefficient and inception. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXGkgz1S7Ak8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(x,y,color = 'red', marker = '+', label = 'Data')\n",
        "plt.plot(x,reg.predict(x) , label = 'Linear')\n",
        "plt.xlabel('Hourse')\n",
        "plt.ylabel('Pass Exam (0:pass,1:fail)')\n",
        "\n",
        "import numpy as np\n",
        "x_sigmoid = np.sort(np.array(x),axis=0)\n",
        "##plot the logistic regression\n",
        "plt.plot(x_sigmoid,logreg.predict(pd.DataFrame(x_sigmoid)), label = 'Logitic', color ='orange')\n",
        "\n",
        "import math\n",
        "## plot the sigmoid function\n",
        "a=float(logreg.coef_)\n",
        "b=float(logreg.intercept_)\n",
        "y_sigmoid = [1 / ( 1 + math.exp(-1 * ( a * val + b ))) for val in x_sigmoid]\n",
        "plt.plot(x_sigmoid,y_sigmoid, label = 'Sigmoid', color ='green')\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAgRJfnHLayp",
        "colab_type": "text"
      },
      "source": [
        "We could also add the probability of classification for each point in the datset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7baRcAetJYV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(x,y,color = 'red', marker = '+', label = 'Data')\n",
        "plt.plot(x,reg.predict(x) , label = 'Linear')\n",
        "plt.xlabel('Hourse')\n",
        "plt.ylabel('Pass Exam (0:pass,1:fail)')\n",
        "\n",
        "import numpy as np\n",
        "x_sigmoid = np.sort(np.array(x),axis=0)\n",
        "##plot the logistic regression\n",
        "plt.plot(x_sigmoid,logreg.predict(pd.DataFrame(x_sigmoid)), label = 'Logitic', color ='orange')\n",
        "\n",
        "import math\n",
        "## plot the sigmoid function\n",
        "a=float(logreg.coef_)\n",
        "b=float(logreg.intercept_)\n",
        "y_sigmoid = [1 / ( 1 + math.exp(-1 * ( a * val + b ))) for val in x_sigmoid]\n",
        "plt.plot(x_sigmoid,y_sigmoid, label = 'Sigmoid', color ='green')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "## geting probaility of classification for each point in the datset\n",
        "y_prob = logreg.predict_proba(x)\n",
        "plt.scatter(x,y_prob[:,1],color = 'black', marker = '+', label = 'prob')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg0bQjS5CMlB",
        "colab_type": "text"
      },
      "source": [
        "We observe the following from the above figure: \\\\\n",
        "1- Sigmoid fits the dataset better than the linear regression. It's values are between 0 and 1. \\\\\n",
        "2- Linear regression extend to values less than zero and more than one which doesn't match the dataset (pass=1, fail=0). \\\\\n",
        "3- The logistic curve is zero when the sigmoid is less than 0.5, and the logistic curve is one when the sigmoid is more than 0.5. \\\\\n",
        "4- The sigmoid curve is the probability of classification. \\\\ \n",
        "\n",
        "Also for logistic regression, the intercept (b) moves the curve left and right and the slope (a) defines the steepness of the curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH1COvVuXwR5",
        "colab_type": "text"
      },
      "source": [
        "# Case #2: HR Analysis\n",
        "\n",
        "In this section, we will analyze the data of employees of a company. This data includes some information about the employees who are working at the company and those who left the company. Our objective is to predict whether an existing employee would leave the company based on his/her current status. This will help us decide to offer the employee some incentives to keep him/her in the company. This could also be used to plan early to hire new employees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eR70Y6nbQYs",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Read the input data from the csv file (HR_comma_sep.csv) file. Dataset is downloaded from Kaggle. Link: https://www.kaggle.com/giripujar/hr-analytics. Use the pandas library (https://pandas.pydata.org/) to read the data from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZrjhWA3YRcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "HR = pd.read_csv('./MachineLearning/2_logistic/HR_comma_sep.csv')\n",
        "HR.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEVwPozTbhm6",
        "colab_type": "text"
      },
      "source": [
        "Before applying regression to the data, we will explore and analyze the data to determine the features that influence the decision of the students to remain or leave the company."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PfSFCycBBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "left = HR[HR.left==1] ## employees who left the company \n",
        "No_left= left.shape[0]\n",
        "remain = HR[HR.left==0] ## employees who remain at the company \n",
        "No_remain = remain.shape[0]\n",
        "Per_left = No_left / (No_left + No_remain)\n",
        "\n",
        "print('No_left = {}, No_remain = {} , Percentage of left = {} %'.format(No_left,No_remain,Per_left*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R61IR-gJdNQQ",
        "colab_type": "text"
      },
      "source": [
        "About $23\\%$ employees left the company. Now, let us check which features are mostly affecting the decision of employees to leave or remain in the company. To do this, we will measure the average of each numeric feature for employees to remain or leave the company.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmI7sVO4d6vG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HR.groupby('left').mean() #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY7uKIrGeOIQ",
        "colab_type": "text"
      },
      "source": [
        "We may conclude the following from the table above: \\\\\n",
        "1- Employees who remain in the company has higher satisfaction_level and thus it is a good indicator for our regression/classifier (good feature) \\\\\n",
        "2- The last_evaluation, number of projects, and time_spend_company scores are almost independent of the employees remain or leave the company \\\\\n",
        "3- The average_montly_hours for employees who left the company are higher than those who remained which could be an indicator (good feature) \\\\\n",
        "4- The promotion_last_5years feature of employees remaining in the company is much higher than those left the company (good feature) \\\\\n",
        "5- Work_accident is also an indicator so it is a good feature.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MlXIibs9Z8R",
        "colab_type": "text"
      },
      "source": [
        "Let us also check the quality of the categories' features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Rf5SjP6Ix8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.salary,HR.left)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWOZE5NBy5ru",
        "colab_type": "text"
      },
      "source": [
        "The salary table shows that emloyees with high salaries are more likely to stay in the company. So it is a good feature. To visualize this we make a bar plot as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xIPIbJ5965b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.salary,HR.left).plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q0QfTZjzPs2",
        "colab_type": "text"
      },
      "source": [
        "We need also to investigate the department feature as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCHfkCyz-B-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(HR.Department,HR.left).plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT1gQWr30YT0",
        "colab_type": "text"
      },
      "source": [
        "The department type has a minor effect on the decision of employees to stay or leave the company. It doesn't look a major factor and thus we will ignore this feature. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3okSyzEL8HzB",
        "colab_type": "text"
      },
      "source": [
        "Based on the above analysis, we will create the following table which includes only the good (important, major) features affecting employees decisions to stay or leave the company"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2I6qW3L5XS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HR_GF = HR[['left','satisfaction_level','average_montly_hours','Work_accident','promotion_last_5years', 'salary']]\n",
        "HR_GF.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ho-hLuj1bTz",
        "colab_type": "text"
      },
      "source": [
        "To prepare the data for classification (using logistic regression), we will apply one hot encoding for the categorical features (salary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnY3Rx-V11Dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dm = pd.get_dummies(HR_GF.salary)\n",
        "HR_GF_merged = pd.concat([HR_GF,dm],axis=1)\n",
        "HR_GF_merged = HR_GF_merged.drop(['salary','medium'],axis=1)\n",
        "HR_GF_merged.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYVPPi0N4AR_",
        "colab_type": "text"
      },
      "source": [
        "Let us define input (x) and output (y) of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ercG4Iwd4A-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = HR_GF_merged.drop('left',axis=1)\n",
        "y = HR_GF_merged.left"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfEJ_b-i3Vee",
        "colab_type": "text"
      },
      "source": [
        "Before classification, we need to split the datset into test and training parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTFsz9D_3oF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "print('size of test dataset = {}, size of traing data = {}, percentage = {}%'.format(len(x_test),len(x_train),len(x_test)*100/(len(x_test) + len(x_train))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRyJhrij4oAB",
        "colab_type": "text"
      },
      "source": [
        "Now, we are ready to apply logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wVAxuJm4pMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "logreg = linear_model.LogisticRegression()\n",
        "logreg.fit(x_train,y_train)\n",
        "print(logreg.coef_)\n",
        "print(logreg.intercept_)\n",
        "print(logreg.score(x_train,y_train))\n",
        "print(logreg.score(x_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMLQxxK36HRx",
        "colab_type": "text"
      },
      "source": [
        "Results show that $78\\%$ testing accuracy is achieved which is very close to the training accuracy ($77.3\\%$)."
      ]
    }
  ]
}